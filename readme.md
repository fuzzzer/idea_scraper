# Reddit Scraper â€” quick start

A tiny command-line tool that downloads every **submission + full comment tree**
from a subreddit during a date range.  
Output = newline-delimited JSON (and, if you want, CSVs / per-post TXT files
and a single _merged_ TXT)

---

## 1â€ƒPrerequisites

| What                         | Why                                                               |
| ---------------------------- | ----------------------------------------------------------------- |
| Python â‰¥ 3.9                 | runtime                                                           |
| A **Reddit script-type app** | yields the `client_id` and `client_secret` you must put in `.env` |

### How to create the Reddit app (60 sec)

1. Log in on reddit.com and open **[prefs / apps](https://www.reddit.com/prefs/apps)**
2. Click **â€œCreate appâ€**
3. Choose **script**
4. Fill **name** (anything) & **redirect URI** â†’ `http://localhost` (unused)
5. Click **Create** â€“ youâ€™ll see  
   â€¢ a 14- or 22-char string under the app name â†’ **client ID**  
   â€¢ a long **secret**
6. Keep that tab open; youâ€™ll copy both into `.env`.

---

## 2â€ƒInstall (editable dev mode)

```bash
git clone https://github.com/your-org/reddit_scraper.git
cd reddit_scraper
python -m venv .venv && source .venv/bin/activate      # Windows: .venv\Scripts\activate
python -m pip install -U pip
python -m pip install -e .                             # pulls project dependencies
```

---

## 3â€ƒCredentials

Create a tiny **`.env`** file in the project root:

```dotenv
REDDIT_CLIENT_ID=xxxxxxxxxxxxxx                 # â† copy the ID
REDDIT_CLIENT_SECRET=yyyyyyyyyyyyyyyyyyyyyyyyyy # â† copy the secret
REDDIT_USER_AGENT=reddit_scraper/0.1 by <your_username>
```

Thatâ€™s it â€“ the scraper autoloads the file.

---

## 4â€ƒRun a scrape

```bash
# syntax: reddit-scraper <subreddit> <start> <end> [flags]
python -m reddit_scraper.cli learnpython 2025-06-15 2025-06-20 \
  --min-score 2 \
  --csv \
  --txt \
  --merged
```

### Flag cheat-sheet

| Flag                      | Meaning / side effect                                        |
| ------------------------- | ------------------------------------------------------------ |
| `--min-score N`           | skip posts with score < N                                    |
| `--flair "A,B"`           | include only those flairs (comma-sep, case-insensitive)      |
| `--csv`                   | export two flat CSVs (`*_submissions.csv`, `*_comments.csv`) |
| `--txt`                   | export **per-post** TXT conversations                        |
| `--merged` (+ `--txt`)    | also create one big TXT with all conversations               |
| `--progress-db my.sqlite` | alternate checkpoint DB                                      |
| `--log-level DEBUG`       | verbose logging                                              |

> Re-run the **same command** at any time; already-saved IDs are skipped.

### Where the artefacts land

All outputs go under **`outputs/`**:

```
outputs/
â”œâ”€â”€ data/                output_<sub>_<start>__<end>.ndjson
â”œâ”€â”€ progress/            progress_<sub>_<start>__<end>.sqlite
â”œâ”€â”€ csv/                 (only if --csv)  *_submissions.csv / *_comments.csv
â””â”€â”€ txt/
    â”œâ”€â”€ conversations_<sub>_<start>__<end>/   # one TXT per post  (if --txt)
    â””â”€â”€ all_conversations_<sub>_<start>__<end>.txt   # merged (if --merged)
```

---

## 5â€ƒSample one-liner

```bash
python -m reddit_scraper.cli learnpython 2025-06-20 2025-06-20 --min-score 5 --txt
```

Look in `outputs/` for the freshly created files.

Happy scraping ğŸ‰
